{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lesson: hazard, vulnerability and exposure during Hurricane Katrina\n",
    "\n",
    "This workshop focuses on using vulnerability and exposure data to analyze the impacts of [Hurricane Katrina](https://en.wikipedia.org/wiki/Hurricane_Katrina) on the city of New Orleans and its surroundings. This final workshop of the unit is the closest to a **real-world climate risk analysis** for a past extreme event—sometimes called a **\"hindcast\"**.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "This lesson is... quite dense. It introduces more advanced programming tools for overlaying vulnerability and exposure data with hazard data (in this case, a flood extent map). However, <b>you don't need to understand all of it</b>. The assignment will only require you to copy and adapt small parts of this code. \n",
    "\n",
    "We provide these advanced tools in the hope that they might be useful in your future work or in your dissertation.\n",
    "</div>\n",
    "\n",
    "**Learning outcomes:**\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- have a good overview of the fundamental concepts of climate risk assessments, including hazard identification, exposure, and vulnerability.\n",
    "- learn how to use familiar and new Python libraries to process and analyze geospatial flood data.\n",
    "- create informative visualizations of flood extents and damage to interpret potential impacts effectively.\n",
    "- be able to integrate population datasets to estimate the number of individuals affected by varying hazard levels.\n",
    "\n",
    "**Copyright notice:** [Hamish Wilkinson](https://www.linkedin.com/in/hamish-wilkinson-87b34a185) ran the flood simulations, prepared all the data and exercises, and wrote an initial draft of this notebook. Fabien Maussion reviewed and streamlined it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## The flood data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "We use a simulation of the extent of the flood simulated by Hamish at [Fathom](https://www.fathom.global) using the open-source [SFINCS](https://www.deltares.nl/en/software-and-data/products/sfincs) model. This is the only dataset from this lecture which is not openly available online. While it would be possible for you to run such a simulation, this is more for a dissertation topic than a class workshop.\n",
    "\n",
    "**Animation of the flood:** (to be discussed in class)\n",
    "\n",
    "<img src=\"https://cluster.klima.uni-bremen.de/~fmaussion/teaching/qcr/floods/animation.gif\" width=\"600\" alt=\"Flood Animation\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Python packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Let's start by importing the packages. We have a new package this week!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages - these are not new\n",
    "import os\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These are new\n",
    "import rioxarray as rioxr  # Open geotiffs with xarray\n",
    "from rasterio.features import rasterize  # Convert vector data to raster (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    Like last week, this workshop requires the installation of new python packages. This needs to be done once on the machine you are running this notebook. If you haven't done it last week, you should do it today. You'll find the instructions below.\n",
    "</div>\n",
    "\n",
    "**Installing additional packages in an existing environmnent is easy:** \n",
    "\n",
    "1. Start by opening a prompt and activating the `qcr` environment as usual: `conda activate qcr`\n",
    "2. Now, install the requested packages. Let's install a few at a time: type `mamba install --channel conda-forge cftime geopandas rioxarray` (or, if `mamba` is not available, `conda install --channel conda-forge cftime geopandas rioxarray`). Answer yes to the questions.\n",
    "3. That's it! you can now start jupyter-lab as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Go to the [download page](https://fabienmaussion.info/climate_risks/ready/03-download.html#flood-workshop-data) to download all datasets at once. Extract the file in your `data` folder.**\n",
    "\n",
    "The data provided has been preprocessed to make it easier for you to use (mostly - cleaning out unnecessary variables, reprojecting, and cropping large datasets to the study region). However, if you are interested in the process (or indeed if it is helpful for your dissertation) the preprocessing scripts are also provided [here (projection, cropping)](https://github.com/fmaussion/climate_risks/blob/main/book/ws08/00-preprocess-flood-data.py) and [here (data reduction)](https://github.com/fmaussion/climate_risks/blob/main/book/ws08/00-preprossed-flood-fabien.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Hazard: SFINCS flood layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "You will find 3 flood footprints data in the `sfincs_simulations` folder. We are going to work with the historic data for now, while the future scenarios will be used for your assignment. Let's open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the historic event and examine it.\n",
    "ds_hist = xr.open_dataset('../data/flood/sfincs_simulations/hindcast_sfincs_map.nc')\n",
    "ds_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "**E: Explore the file (variables, coordinates, attributes). Which of the variables are gridded variables? Which variable also has a time dimension?**\n",
    "\n",
    "We will now focus on `zsmax` variable for further analysis. Other variables are provided for documentation, if you are interested.\n",
    "\n",
    "**E: explore the `zsmax` variable. What is the units of the variable. Its \"long name\", and \"standard name\"? Plot an image of the data for timestamp `'2005-08-29 18:00'`** *Hint: `ds_hist.zsmax.sel(timemax='2005-08-29 18:00')`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "`zsmax` is the simulated maximal flood depth for each timestamp. It represents the evolution of the flood depth above ground during the Hurricane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Bonus: animate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The code below is \"just for fun\" - nothing quantitative happening there!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Create a figure and axes to plot on\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Coarsen the data by a factor of 10 in both x and y directions\n",
    "# This reduces the resolution to make the animation computationally manageable\n",
    "zsmax = ds_hist['zsmax'].coarsen(x=10, y=10, boundary=\"trim\").mean()\n",
    "\n",
    "# Plot the initial frame of the animation\n",
    "cax = zsmax.isel(timemax=0).plot.imshow(ax=ax,  # Use the predefined axes\n",
    "                                        add_colorbar=True,  # Display a colorbar for reference\n",
    "                                        cmap='viridis',  # Set the colormap to 'viridis'\n",
    "                                        vmin=0, vmax=zsmax.max()  # Define color scale limits\n",
    "                                        )\n",
    "ax.axis('equal')  # Ensure aspect ratio is maintained\n",
    "\n",
    "# Define the animation function, which updates the frame at each timestep\n",
    "def animate(frame):\n",
    "    # Update the title with the current time (formatted as YYYY-MM-DD HH:mm)\n",
    "    ax.set_title(f'Time: {pd.to_datetime(str(zsmax.timemax.data[frame])).strftime(\"%Y-%m-%d %H:%M\")}')\n",
    "    # Update the color array with the new frame's data\n",
    "    cax.set_array(zsmax.values[frame, :])\n",
    "\n",
    "# Create the animation object\n",
    "ani_glacier = animation.FuncAnimation(fig, animate, frames=len(zsmax.timemax), interval=100)\n",
    "\n",
    "# Close the figure to prevent it from displaying in the cell output\n",
    "plt.close(fig)\n",
    "\n",
    "# Display the animation as an interactive HTML object\n",
    "HTML(ani_glacier.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### Max flood extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The time dimension of the flood is interesting to better grasp what the data represents. It's also useful to understand the processes behind the flood and, if these were actual forecasts (and not hindcasts), these could provide tools for rescue services. \n",
    "\n",
    "For flood damage analysis, however, we are typically interested in the maximal flood footprint (the extent of the maximum flood) and the maximal flood depth (the maximal amount of water experienced by infrastructure and populations), regardless of the time at which it occurs. \n",
    "\n",
    "So lets compute this variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flood_depth = ds_hist.zsmax.max(dim=\"timemax\")\n",
    "\n",
    "# Plot it\n",
    "max_flood_depth.plot.imshow();  # Note the use of imshow() instead of .plot() - this is to speedup the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "You may have noticed that plotting this data took a little while longer than you are used to. This is because this data is at a higher resolution than the climate data we have worked with so far. The data is still in lat/lon. \n",
    "\n",
    "**E: What is the spatial resolution in lat/lon? Can you convert this to meters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The plot above shows us max water depth above the reference (in this case, the elevation model used in the simulation). \n",
    "\n",
    "While depth information is useful for a whole host of reasons (we will look into depth damage curves later), to start with we are going to keep it simple and work off the flood extent first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_flood_extent = ~ max_flood_depth.isnull()\n",
    "max_flood_extent.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "**Q: What is the line of code `~ max_flood_depth.isnull()` doing?** Have a look at both the plots and the data if you are not sure, and try with and without the `~`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Bonus: plot the data on a georeferenced map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The code below is giving you tools to plot regional data on a map, should you want to do it in your dissertation.\n",
    "</div>\n",
    "\n",
    "The plot above may be quite difficult to understand without coastlines, etc. Here is a code snipped for you to use if you want prettier plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Create figure and axis with UTM projection\n",
    "fig, ax = plt.subplots(subplot_kw={'projection': ccrs.Mercator()})\n",
    "\n",
    "# Plot the max flood depth\n",
    "max_flood_depth.plot(ax=ax,\n",
    "                     transform=ccrs.PlateCarree(),  # Data is in lat/lon, so we need to transform it\n",
    "                     cmap='Reds',  # Optional: Adjust colormap\n",
    "                     vmin=0,\n",
    "                     add_colorbar=True)\n",
    "\n",
    "# Get dataset coordinate bounds for setting extent\n",
    "lon_min, lon_max = max_flood_depth.x.min(), max_flood_depth.x.max()\n",
    "lat_min, lat_max = max_flood_depth.y.min(), max_flood_depth.y.max()\n",
    "# Set extent dynamically based on dataset bounds\n",
    "ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n",
    "\n",
    "# Add Cartopy features: coastlines, U.S. states, and rivers\n",
    "ax.coastlines()\n",
    "ax.add_feature(cfeature.STATES, linestyle='-', edgecolor='black')\n",
    "ax.add_feature(cfeature.BORDERS, linestyle=':', edgecolor='gray')  # Country borders\n",
    "ax.add_feature(cfeature.RIVERS, edgecolor='blue', alpha=0.5)  # Adds major rivers\n",
    "\n",
    "# Add gridlines (optional)\n",
    "gl = ax.gridlines(draw_labels=True, linestyle=\"--\", alpha=0.5)\n",
    "gl.right_labels = False\n",
    "gl.top_labels = False\n",
    "\n",
    "plt.title('Max flood depth');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Summary: SFINCS data\n",
    "\n",
    "We have a map of maximum flood depth and flood extent. These maps are generated using a numerical model, which is typically validated to ensure it represents real-world conditions accurately. In this case, these simulations have been run specifically for this workshop — let’s just say that these maps are “close enough” for our purposes today.\n",
    "\n",
    "Using this flood extent, we can perform **\"hit\" analyses** on population and economic data. To simplify your work, we have preprocessed population and economic data for you by cropping it to match the flood simulation map and arranging it for easier interpretation. \n",
    "\n",
    "If you're interested, you can check out the [preprocessing script](https://github.com/fmaussion/climate_risks/blob/main/book/ws08/00-preprocess-flood-data.py) and the raw data. Unfortunately, the preprocessing is a bit complex — as we've already mentioned, real-world data is rarely as clean or structured as we'd like it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Exposure: Present-day population data\n",
    "\n",
    "One of the simpliest ways to analyse the impact of a flood is to perform a simple intersection of a population map with flood extent and exposure datasets. \n",
    "\n",
    "Let's read in a population dataset to do just that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we read in population data\n",
    "worldpop_hist = rioxr.open_rasterio(\"../data/flood/population/cropped_usa_ppp_2020_constrained.tif\", masked=True)\n",
    "worldpop_hist = worldpop_hist.squeeze() # Get rid of unneeded band dimension\n",
    "worldpop_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "You may have noticed that the [WorldPop](https://www.worldpop.org) data file has a different file suffix than other data we have worked with. This is because it is a **[GeoTIFF](https://en.wikipedia.org/wiki/GeoTIFF)**. A GeoTiff is essentially an image that is georeferenced to a specific location (and coordinate system) on Earth. In this way, it is somewhat similar to a NetCDF file, as both formats store spatial data. However, GeoTiffs are typically less complex than NetCDFs. They often contain only a single variable (referred to as a \"band\" in GeoTiffs), whereas NetCDF files can store multiple variables, dimensions, and metadata.\n",
    "\n",
    "Conveniently, xarray can work with GeoTiffs, allowing us to read and manipulate the data in much the same way as a NetCDF file. The key difference is that we read it in as a DataArray using rasterIO rather than a Dataset, since it contains only a single variable.\n",
    "\n",
    "For all other purposes, once read, the data should look familiar to you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "worldpop_hist.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "You can see from the above plot that WorldPop shows the spatial distribution of population. In essence this is a process of disaggregation. WorldPop has taken information on population from sources such as censuses and disagregatted this to areas where is algorithms consider it most likley that people will live. This is a goldmine for impact analysis, as it allows us a much more granular picture of risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Exposure: Projections of population data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Lets consider another population dataset - in this case a projection of future population under two SSP scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and examine the data\n",
    "pop_proj = xr.open_dataset(\"../data/flood/population/cropped_ICLUS_v2_1_1_population_stripped.nc\")\n",
    "pop_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to get and idea of the spatial distribution.\n",
    "pop_proj[\"TOTALPOP10\"].plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "This first thing you will probably notice is that the spatial layout of this data appears both more complete, and more discrete than the SFINCS and WorldPop data we have looked at so far. This is because this data is much coarser - it shows the total population of adminstrative regions, and the total projected population under future SSP scenarios. However, it makes no effort to disaggregate this information like WorldPop. In fact it originally was in vector format. I have converted it to NetCDF here so you can easily use it for analysis later in your assignment. If you want to have a look at the original data it is in the raw data folder.\n",
    "\n",
    "**Q: Have a look at the other variables. We have population in 2010, and population in 2080 under the two SSPs. What do you think the scaler is? Why might this be useful when combined with a more granular dataset such as WorldPop?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Exposure : Economic assets\n",
    "\n",
    "So far we have conecentrated on population, which is of course is a large aspect of what we consider in impact studies. However, another important aspect is the cost of economic damages in relation to flooding. We can estimate this using the [National Structures Inventory (NSI)](https://gee-community-catalog.org/projects/nsi/), a US dataset which lists a value for each property. Let's open the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi = xr.open_dataset(\"../data/flood/demographics/cropped_NSI.nc\")\n",
    "nsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Unfortunately the data variables have no units or metadata. We'll have to find out what's in it by ourselves: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi.total_val.plot.imshow(); # This shows the aggregate value of housing in each gridcell - in $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsi.properties.plot.imshow(); # This shows the total number of properties in each cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Examining these datasets, you will notice that they have a similar granularity to the WorldPop data. In fact, you might assume they were originally raster datasets. However, that is not the case—these datasets were originally vector data, specifically point-based rather than polygon-based.\n",
    "\n",
    "If you're curious about the conversion process, you are encouraged to check out the [preprocessing script](https://github.com/fmaussion/climate_risks/blob/main/book/ws08/00-preprocess-flood-data.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "### Vunerability: deprivation index\n",
    "\n",
    "The [Area Deprivation Index (ADI)](https://www.nrpa.org/publications-research/data-and-mapping-resource-library/area-deprivation-index/) is a deprivation index for the USA. It was originally in `csv` format. I combined it with a census polygon vector dataset to make it spatial. For full detail see the [preprocessing script](https://github.com/fmaussion/climate_risks/blob/main/book/ws08/00-preprocess-flood-data.py).\n",
    "\n",
    "We provide ADI as a `gpkg`, which is short for [GeoPackage](https://en.wikipedia.org/wiki/GeoPackage). It's very much like the shapefiles you analysed last week, but in a more open format (and without the drawback of having multiple files).\n",
    "\n",
    "`gpkg` are opened with GeoPandas like shapefiles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adi = gpd.read_file(\"../data/flood/demographics/cropped_US_2022_ADI_Census_Block_Group_v4_0_1.gpkg\")\n",
    "df_adi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Note that `ADI_NATRANK` is the national score of deprivation, and `ADI_STATERNK` is specific to that state. We are working across state lines, so it would be best to use the national rank in our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adi.plot(column=\"ADI_NATRANK\", cmap=\"RdYlGn_r\", legend=True);  # this is vector data (not raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "Note that we have reversed the colour bar `_r`). That is because zero is least deprived, while red is most deprived. You can find more information on this in the README file provided in the data folder. \n",
    "\n",
    "**Q: What spatial patterns jump out at you? How do you think this might translate to flood risk vunerability? Further, there are some white spaces - why is this data missing?** *Hint: The readme may help you here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### Rasterisation of vector data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "ADI data is provided in vector format - more specifically a polygons geodataframe. While we can use this with xarray data using intersections and rioxarray, it is not very computationally efficent. Instead, lets rasterise it.\n",
    "\n",
    "Arguably this could have been part of our preprocessing step - but we decided to let you have a go at it yourselves, so you know it's (relatively) easy to do.\n",
    "\n",
    "The key points in the below code is that we are taking the polygon geometry, and everywhere this touches we are burning the value held within the `ADI_NATRANK` column into the pixels of the spatially underlying grid.\n",
    "\n",
    "To get this grid we use a template that we want to compare it to. In this case WorldPop. The shape gives the columns and rows of the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code converts the polygons we have to the same grid that our other data is in. This allows pixel by pixel comparisions.\n",
    "adi = rasterize(\n",
    "    shapes=zip(df_adi.geometry, df_adi[\"ADI_NATRANK\"]),  # Burn the column values using vector geometry.\n",
    "    out_shape=worldpop_hist.rio.shape,        # Shape (rows/columns) of the output raster from our template (WorldPop)\n",
    "    transform=worldpop_hist.rio.transform(),  # Affine transform from the template raster (Don't worry about this too much).\n",
    "    fill=np.nan,                         # Background value (non numeric so we dont screw calcs)\n",
    "    all_touched=True,                    # Include all pixels touched by geometries\n",
    "    dtype=np.float32                     # Data type of the output raster\n",
    ")\n",
    "\n",
    "# Transform to xarray.\n",
    "adi = xr.DataArray(adi, dims=(\"y\", \"x\"), coords={\"y\": worldpop_hist.y, \"x\": worldpop_hist.x})\n",
    "\n",
    "# Lets make sure our data looks as we would expect.\n",
    "adi.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "NB we have some missing data. If we were feeling clever we could interpolate over the gaps. For this exercise we will just leave them blank - they will be exclude from our analysis as they are `np.nan`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "**E: verify that `max_flood_extent`, `max_flood_depth`, `worldpop`, `adi` all have the same dimensions.** This is often a prerequisite for many data analysis pipelines - the data needs to be homogenised, normally on a common grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### Vulnerability ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Finally, we note that the categories for vulnerability are very fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(adi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "That is perhaps too fine for our liking. Let's coarsen and round to the nearest 10. We are going to use a simply trick to convert these into increments of 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounds each value in 'adi' to the nearest multiple of 10\n",
    "adi.data = np.round(adi / 10) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "This is now cetgorized in simpler ranks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(adi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "adi.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "Same data - coarser categories!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## The Flood Risk Analysis – Finally!\n",
    "\n",
    "We have now read in all our data. More importantly, we have visualized and explored it, giving us a reasonable understanding of the data we are working with — its structure, characteristics, and potential shortcomings.\n",
    "\n",
    "Note that I say \"reasonable\" — perhaps \"superficial\" would be a more accurate description. In general, I strongly recommend reading research papers about a dataset before using it. This helps you better understand the assumptions and compromises made during its generation process.\n",
    "\n",
    "That said, we now know enough to move forward — so let's dive into some analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### Flood extent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "For the purpose of this exercise, let's assume that each grid point has an approximate area of 0.036 km$^2$.\n",
    "\n",
    "**E: compute the total area (in km$^2$) of the flood (using `max_flood_extent`) and the pixel area given above.** Compare this to the area of the UK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Population exposure\n",
    "\n",
    "First lets look at the numbers of people effected by flooding according to WorldPop.\n",
    "\n",
    "#### Exposure by flood extent\n",
    "\n",
    "We start by asking a simple question: how many people where impacted by the flood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This uses the extent mask we made earlier to keep only WorldPop values impacted by flooding.\n",
    "flooded_worldpop = worldpop_hist.where(max_flood_extent)\n",
    "\n",
    "# This shows us where - on the coast as expected. I always find it worth\n",
    "# plotting data to make sure the code is doing what I am expecting:\n",
    "flooded_worldpop.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "**Q: What does `worldpop.where(max_flood_extent)` achieve? If you can't figure it out, plot the `worldpop` and `max_flood_extent` separately, and then the outcome (`flooded_worldpop`).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "NB - if you wanted to make plots for including in a report or dissertation you might want to include mapping elements, or even just outlines of coast and states to give the reader more context. Here though we are engaged in prelimary analysis - we can tidy up plots later if we need to. See the example above on how to do that.\n",
    "\n",
    "**E: some quantitative analysis now! Answer the following questions:**\n",
    "- **what is the higher population value in a flooded cell?**\n",
    "- **what is the total number of people affected by the flood?**\n",
    "- **compare this number with estimates of people displaced or left homeless by the flood (web search). Does it (roughly) add up?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "#### Exposure by flood depth categories\n",
    "\n",
    "The full number of people is an important analysis - but flooding is not happening equally everywhere. What if we wanted to know how many people are affected by different levels of flooding? \n",
    "\n",
    "This can be done by a proccess called binning - very similar to histograms: we want to create categories (e.g. <0.5 m flooding, between 0.5 and 1.5, etc.) and then figure out the extent of the flood for each category, and then figure out how many people are affected by each category. Does that make sense? Let's prepare our bins first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bins\n",
    "bins = [0, 0.5, 1.5, 2.5, 3.5, 4.5, 5.5, np.max(max_flood_depth).item()]\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "Next, we will use [np.digitize](https://numpy.org/doc/2.1/reference/generated/numpy.digitize.html) to classify our flood depth into categories. We want to work with xarray still (digitize is a numpy function), so we'll just ask xarray to apply np.digitize to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = max_flood_depth.copy()  # Just create a container for the data\n",
    "categories.data = np.digitize(max_flood_depth, bins, right=True)  # Categorize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Let's have a look at our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "We have **9 categories**:  \n",
    "- **0** (< 0m flood depth)  \n",
    "- **1** (0–0.5m flood depth)  \n",
    "- ...  \n",
    "- **7** (> 5.5m flood depth)  \n",
    "- **8** (missing values)  \n",
    "\n",
    "Let's rearrange the data so that 0 represents \"no flood,\" while the remaining values correspond to different flood depth categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_category_map = categories.where(categories != categories.max(), 0)  # Replace category 9 with 0\n",
    "flood_category_map.plot.imshow();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Now that's very useful. We now have a categorical map of the flood severity (note that category 1, <0.5m, is already quite severe!).\n",
    "\n",
    "We can use this information to find out how many people where affected by the categories 3 and above. Let's go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to masking above - but this time we have a smaller extent for all categories > 3\n",
    "flooded_cat3_and_above = worldpop_hist.where(flood_category_map >= 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "**E: plot the `flooded_cat3_and_above` variable. Now compute the total number of people affected by these categories, as a total number and as a percentage of teh total number of people affected by the flood (any category). Now repeat with category 6 and above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Good - that looks about like we expect: a smaller amount of people with each increase in flood depth. \n",
    "\n",
    "With this information we can now check out the split of people effected by different depths of flooding. Let us write this code for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets create a dataframe to store our results\n",
    "impacts_df = pd.DataFrame()\n",
    "\n",
    "# Now lets loop over our extents to pull out effected pop\n",
    "for cat in sorted(np.unique(flood_category_map)):\n",
    "\n",
    "    # This is a bit of ugly code to add a legend to the categories\n",
    "    # dont worry too much about it\n",
    "    if cat == 0:\n",
    "        continue  # We dont care about this category\n",
    "    elif cat == 1:\n",
    "        depth = f'0-{bins[cat]}m'\n",
    "    elif cat == flood_category_map.max():\n",
    "        depth = f'>={bins[cat-1]}m'\n",
    "    else:\n",
    "        depth = f'{bins[cat-1]}-{bins[cat]}m'\n",
    "\n",
    "    impacts_df.loc[cat, 'Depth'] = depth\n",
    "\n",
    "    # Masking as we did before with an extent.\n",
    "    pop_affected = worldpop_hist.where(flood_category_map == cat).sum().item()\n",
    "\n",
    "    # Also compute the area\n",
    "    area_affected = (flood_category_map == cat).sum().item()\n",
    "\n",
    "    # Store the data\n",
    "    impacts_df.loc[cat, 'Pop_affected'] = pop_affected\n",
    "    impacts_df.loc[cat, 'Area_affected'] = area_affected\n",
    "\n",
    "# Lets have a look\n",
    "impacts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "I note we still have fractions of people. It takes away from our analysis a bit. Lets round them out and create a nice visual representation of our depth exposure profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_df[\"Pop_affected\"] = impacts_df[\"Pop_affected\"].astype(int)\n",
    "\n",
    "impacts_df.plot(kind=\"bar\", x=\"Depth\", y=\"Pop_affected\"); # Again we can make a prettier plot than this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_df.plot(kind=\"bar\", x=\"Depth\", y=\"Area_affected\"); # Again we can make a prettier plot than this!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "**Q: analyse and interpret the plots above. How does area correlate to population affected?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### Vulnerability\n",
    "\n",
    "Now that we have estimated the number of people affected, we can expand our analysis. How vulnerable are the affected populations? And how is the risk distributed demographically?\n",
    "\n",
    "There are several ways to approach this analysis. Here, we address one research question:  \n",
    "For a given flood depth category, what proportion of deprived areas is affected?\n",
    "\n",
    "Let's start simple first: Within the flood extent (regardless of depth), what is the socioeconomic distribution of the affected population?  To answer this, we need to overlay the population map with the flood extent and then determine which socioeconomic category each affected population belongs to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by masking population data according to flood extent\n",
    "affected_pop = worldpop_hist.where(max_flood_extent)\n",
    "\n",
    "# Then create two new masks: for high and low deprivation areas\n",
    "affected_pop_high_deprivation = affected_pop.where(adi >= 60)\n",
    "affected_pop_low_deprivation = affected_pop.where(adi < 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "**E: plot the two maps (affected high and low deprivation). Now count the number of people affected for each deprivation category. Did the flood affect more people in depraved or in less depraved areas? What is the ratio between the two values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "Now let's redo this analysis but with another flood category - for example > 1.5 m (cat 3 and above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by masking population data according to flood extent\n",
    "affected_pop_cat3 = worldpop_hist.where(flood_category_map >= 3)\n",
    "\n",
    "# Then create two new masks: for high and low deprivation areas\n",
    "affected_pop_cat3_high_deprivation = affected_pop_cat3.where(adi >= 60)\n",
    "affected_pop_cat3_low_deprivation = affected_pop_cat3.where(adi < 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "**E: count the number of people affected for each deprivation category, this time in the flood category 3. Did the flood affect more people in depraved or in less depraved areas? What is the ratio between the two values? Compare this ratio with the entire flood area computed above.** *Hint: this doesnt change much*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "Note that this does not mean that deprived areas are more likely to be flooded. Addressing this research question properly is not trivial with the data available to us—we would need even more fine-grained data to answer it accurately.  \n",
    "\n",
    "Instead, this analysis serves as a diagnostic tool to quantify the extent of the damage and assess its impact on different populations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "#### Bonus: stacked bar plots\n",
    "\n",
    "The analysis above was done on an ad-hoc basis - defining thresholds for low and high deprivations, and for single categories of flood depth. We can generalise this analysis by looping over all flood depth and vulnerability categories.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "The code below is a bit more advanced in terms of data manipulation. You can skip it if you prefer!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "Let's start by reminding us of our depth categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "flood_categories = impacts_df.index\n",
    "flood_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "And we also have deprivation categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List our vunerability ranks\n",
    "vun_ranks = np.unique(adi)\n",
    "# Remove NaN values\n",
    "vun_ranks = vun_ranks[~np.isnan(vun_ranks)]\n",
    "vun_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "OK so we have 7 depth categories and 11 deprivation categories. Now we are going to loop over all of them in a nested loop: the outer loop for depth, the inner loop for vulnerability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data container\n",
    "vun_df = pd.DataFrame()\n",
    "\n",
    "i = 0  # Just the index\n",
    "\n",
    "# First we loop over our depth extents from earlier\n",
    "for cat, depth in zip(flood_categories, impacts_df['Depth']):\n",
    "\n",
    "    # Isolate the areas effected by this flood depth\n",
    "    affected_pop_cat = worldpop_hist.where(flood_category_map == cat)\n",
    "\n",
    "    # Now we loop over our vunerability ranks for that specfic flood depth extent.\n",
    "    for vun_rank in vun_ranks:\n",
    "\n",
    "        # Same as above - isolate the population data for this rank\n",
    "        affected_pop_rank = affected_pop_cat.where(adi == vun_rank)\n",
    "\n",
    "        # Store the data we have just harvested in the dataframe\n",
    "        vun_df.loc[i, 'depth'] = depth\n",
    "        vun_df.loc[i, 'vun_rank'] = vun_rank\n",
    "        vun_df.loc[i, 'pop'] = affected_pop_rank.sum().item()\n",
    "\n",
    "        # Update counter\n",
    "        i += 1\n",
    "\n",
    "# Have a look\n",
    "vun_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "Quick segway here on nested loops. If you are still struggling to get your head around it, perhaps this will help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer = [1,2]\n",
    "inner = [\"cat\", \"dog\", \"hamster\"]\n",
    "\n",
    "for i in range(len(outer)):\n",
    "\n",
    "    print(\"This is OUTER loop \" + str(i+1))\n",
    "\n",
    "    for j in range(len(inner)):\n",
    "\n",
    "        print(\"This is inner loop \" + str(j+1) + \". Value is \" + inner[j] + \".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116",
   "metadata": {},
   "source": [
    "As you can see for each iteration of the outer loop, in the inner loop completes its full loop. Let us know if this is still confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "Back to our analysis of flood vunerabilty. From the dataframe we can see we have the number of people in each vunerability rank for each flood extent, but its not in a clearly readable format. Lets graph it up using a stacked barchart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "vun_df_stack = vun_df.groupby([\"depth\", \"vun_rank\"])[\"pop\"].sum().unstack()\n",
    "vun_df_stack.plot(kind=\"bar\", stacked=True, colormap=\"RdYlGn_r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "### Economics\n",
    "\n",
    "Having focused on people and deprivation, I hear you crying \"Won't anyone think about the money?!\" Lets use our total flood extent and the NSI layer to find our just how much Katrina costs according to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .where() also works with datasets!\n",
    "econ_dmg = nsi.where(max_flood_extent)\n",
    "\n",
    "total_cost = econ_dmg[\"total_val\"].sum().item()\n",
    "number_prop = econ_dmg[\"properties\"].sum().item()\n",
    "\n",
    "f'{number_prop:.0f} prperties where affected, for a total damage of {total_cost*1e-9:.0f} billion $'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "The damage from this event is a *little* off though - Katrina cost 201.3 billion (says google). \n",
    "\n",
    "We might be off for a few reasons - **Q: write a few thoughts about the approximations in our analyses below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "Part of the reason why, is that a building and all its contents does not evaporate as soon as water touches it. Damage occurs on a spectrum, illustrated by *damage curves*. We won't go into detail here, but in essence, the deeper the water the higher % value of damage. Lets illustrate this with a simple relationship - lets say above 4m is total destruction (probably about right..) and scale that linearly (very unrealistic) from 0 (no damage). Let's add a damage curve to our flood categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_df['Dmg_rate'] = np.append(np.linspace(0.2, 1.0, 5), [1, 1])\n",
    "impacts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our damage curve\n",
    "for cat in impacts_df.index:\n",
    "\n",
    "    # Isolate the areas effected by flood depth\n",
    "    econ_dmg = nsi['total_val'].where(flood_category_map == cat)\n",
    "\n",
    "    # Total damage (no adjustment)\n",
    "    impacts_df.loc[cat, 'Damage'] = econ_dmg.sum().item() * 1e-9\n",
    "\n",
    "# Apply adjustment for damage rate\n",
    "impacts_df['Damage_adjusted'] = impacts_df['Damage'] * impacts_df['Dmg_rate']\n",
    "\n",
    "# Print out\n",
    "impacts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "Now the total damage is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_df.sum()[['Damage', 'Damage_adjusted']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "This is much better - but it is probably very much just by chance!\n",
    "\n",
    "This illustrates the difficulty pricing insurance. Getting the damage curve just right is complex, and relies on information on structure type amonst others. Salinity also makes a difference. Added to that the NSI dataset has been updated since 2005, and so is likely not representative of conditions during Katrina.\n",
    "\n",
    "Again, lets have a look at the damage curve we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "impacts_df.plot(x='Depth', y='Dmg_rate');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "Lets compare that to a curve plot I pinched from some of my colleagues:\n",
    "\n",
    "![](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41467-020-15264-2/MediaObjects/41467_2020_15264_Fig1_HTML.png?as=webp)\n",
    "\n",
    "*Wing, O.E.J., Pinter, N., Bates, P.D. et al. New insights into US flood vulnerability revealed from flood insurance big data. Nat Commun 11, 1444 (2020). https://doi.org/10.1038/s41467-020-15264-2*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "As you can see, pricing risk is an uncertain business - even in the present day."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This workshop equipped you with practical skills to analyze and interpret spatial risk data (here: flood, but it could be another risk as well).\n",
    "\n",
    "You are encouraged to view this lesson as a resource for your future endeavors. While the number of analyses may seem daunting, you can learn at your own pace and return to this resource when necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
